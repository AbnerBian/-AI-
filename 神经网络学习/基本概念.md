## 计算图

下面这段代码产生了两个计算图，每个计算图中定义了一个名字为"v"的变量。
在计算图g1中，将v初始化为0，在计算图g2中，将v初始化为1.可以看到当运行不同计算
图时，变量v的值也是不一样的。tensorflow中的计算图不仅仅可以用来隔离张量和计算，他还
提供了管理张量和计算的机制

~~~~python
#graph生成新的计算图
g1=tf.Graph()
with g1.as_default():
    #在计算图g1中定义变量"v",并设置初始值为0
    v=tf.get_variable("v",initializer=tf.zeros_initializer,shape=[1])

g2=tf.Graph()
with g2.as_default():
    #在计算图g2中定义变量"v",并设置初始值为1
    v=tf.get_variable("v",initializer=tf.ones_initializer,shape=[1])
    
#在计算图g1中读取变量"v"的取值
with tf.Session(graph=g1) as sess:
    tf.initialize_all_variables().run()
    with tf.variable_scope("",reuse=True):
        #在计算图g1中，变量"v"的取值应该为0，所以下面这行会输出[0,]
        print(sess.run(tf.get_variable("v")))
#在计算图g2中读取变量"v"的取值
with tf.Session(graph=g2) as sess:
    tf.initialize_all_variables().run()
    with tf.variable_scope("",reuse=True):
        #在计算图g2中，变量"v"的取值应该为1，所以下面这行会输出[1.]
        print(sess.run(tf.get_variable("v")))
~~~~



## 张量的概念

所有的数据都通过张量的形式来表示。张量可以被简单理解为多维数组

零阶张量表示标量(scalar),第一阶张量为向量(vector)，也就是一维数组。第n阶张量可以理解为一个n维数组，但张量并不是直接采用数组形式，它只是对tensorflow中计算结果的引用。在张量中并没有真正保存数字，它保存的是如何得到这些数字的计算过程。

## 会话的概念

会话(session)来执行定义好的运算。会话拥有并管理tensorflow程序运行时的所有资源。当所有计算完成之后需要关闭会话来帮助系统回收资源，否则会出现资源泄露

~~~~python
#第一种模式

#创建一个会话
sess=tf.Session()
#使用这个创建好的会话来得到关心的运算的结果。比如可以调用sess.run(result),
#来得到张量result的取值
sess.run(....)
#关闭会话使得本次运行中使用的资源可以被释放
sess.close()

#第二种模式

#创建一个会话，并通过python中的上下文管理器来管理这个会话
with tf.Session() as sess:
  #使用这创建好的会话来计算关心的结果
  sess.run(...)
#不需要再调用“Session.close()”函数来关闭会话
#当上下文退出时会话关闭和资源释放也自动完成了

##这既解决了因异常退出时资源释放的问题，同时也解决了忘记调用Session.close函数而产生的资源泄露
~~~~

## 神经网络解决分类问题步骤

- 1.提取问题中实体的特征向量作为神经网络的输入。不同的实体可以提取不同的特征向量
- 定义神经网络的结构，并定义如何从神经网络的输入得到输出
- 通过训练数据来调整神经网络中参数的取值
- 使用训练好的神经网络来测试未知的数据

## 全连接神经网络

相邻两层之间的任意两个节点之间都有连接

## 随机数变量初始化

在tensorflow中声明一个2*3的矩阵变量

~~~~python
weights=tf.Variable(tf.random_normal([2,3],stddev=2))
#tf.random_normal([2,3],stddev=2)会产生一个2*3的矩阵，矩阵中的元素是均值为0，标准差为2的随机数。
~~~~

## tensorflow 变量的两大属性

- 维度(shape)

  维度可以改变，但是很少在实际中这样做

- 类型(type)

  类型在构建之后，将不再改变

## placeholder机制

tensorflow的计算图将会太大，因为每生成一个常量，tensorflow都会在计算图中增加一个节点。一般来说，一个神经网络的训练过程会需要经过几百万轮甚至几亿轮的迭代，这样计算图就会非常大，而且利用率低。为了避免这个问题，tensorflow提供了placeholder机制用于提供输入数据。placeholder相当于定义了一个位置,这个位置中的数据在程序运行时指定。这样在程序中就不需要生成大量常量来提供输入数据，而只需要将通过placeholder传入tensorflow计算图